---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a postdoctoral scientist in Amazon. My primary research interest lies in data mining, natural language processing and machine learning. My current focus include several key areas: NLP with Structural Knowledge, NLP for Science, Information Retrieval, Retrieval-Augmented Generation, Large Language Model Reasoning.  

Prior to Amazon, I received my Ph.D. degree in Computer Science and Engineering at the [University of Notre Dame](https://cse.nd.edu), advised by [Prof. Meng Jiang](http://www.meng-jiang.com/). Before that, I received my Master degree of ECE department at [UIUC](https://ece.illinois.edu).


**<span style="color:red;">I am actively seeking industrial R&D opportunities, as well as Postdoctoral positions, beginning in 2024. I am happy to engage in discussions regarding potential opportunities!</span>**


Research Interest
======
* Knowledge Graph
* Natural Language Processing
* Large Language Models

What's New
======
* \[August 2024\] The [code](https://github.com/QingkaiZeng/CodeTaxo-Pub) of [CodeTaxo](https://www.arxiv.org/pdf/2408.09070) is available now. Feel free to generate your own taxonomy!
* \[August 2024\] Three papers about (1) Taxonomy Expansion via Code Prompting ([CodeTaxo](https://www.arxiv.org/pdf/2408.09070)) (2) Mathematical Reasoning ([ProCo](https://arxiv.org/pdf/2405.14092)) (3) Curruiculum Learning ([PUDF](https://arxiv.org/pdf/2408.05326)) are available on Arxiv now! 
* \[July 2024\] [Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf) was accepted by [The CIKM 2024](https://cikm2024.org/). The [code](https://github.com/QingkaiZeng/Chain-of-Layer) of [Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf) is available now. Feel free to generate your own taxonomy!
* \[February 2024\] Two papers about (1) Entity Linking via leveraging LLMs ([ChatEL (Coming Soon)]()) (2) Mathematical Reasoning ([MinT](https://arxiv.org/pdf/2307.07951.pdf)) were accepted by [LREC-COLING 2024](https://lrec-coling-2024.org/). Huge congrats to [Yifan Ding](https://scholar.google.com/citations?user=WMegVFUAAAAJ&hl=en) and [Zhenwen Liang](https://zhenwen-nlp.github.io/)!
* \[February 2024\] Three papers about (1) Taxonomy Induction via prompting LLMs ([Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf)) (2) personalized LLMs ([OPPU](https://arxiv.org/pdf/2402.04401.pdf)) (3) Entity Linking via leveraging LLMs ([EntGPT](https://arxiv.org/pdf/2402.06738.pdf)) are available on Arxiv now! 
* \[October 2023\] One paper was accepted by [The EMNLP 2023](https://2023.emnlp.org/).
* \[April 2023\] I am joining Tencent AI Lab as a full-time research intern this summer! I will work with Dr. [Linfeng Song](https://freesunshine0316.github.io/) and Dr. [Haitao Mi](https://scholar.google.com/citations?user=G3OMbFSm858C&hl=en). 
* \[May 2022\] One paper was accepted by [The KDD 2022](https://kdd.org/kdd2022/).
* \[May 2021\] One paper was accepted by [The KDD 2021](https://www.kdd.org/kdd2021/).
* \[May 2021\] Our [ICSE 2021](https://conf.researchr.org/home/icse-2021) paper was selected for an ACM SIGSOFT Distinguished Paper Award!


Recent Publications
======

* **Qingkai Zeng**, Yuyang Bai, Zhaoxuan Tan, Zhenyu Wu, Shangbin Feng, Meng Jiang. CodeTaxo: Enhancing Taxonomy Expansion with Limited
Examples via Code Language Prompts, arXiv preprint 	arXiv:2408.09070 (2024). \[[PDF](https://www.arxiv.org/pdf/2408.09070)\] \[[Code](https://github.com/QingkaiZeng/CodeTaxo-Pub)\]

* **Qingkai Zeng**, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang. Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples, In Proceedings of the ACM International Conference on Information & Knowledge Management (**CIKM**). \[[PDF](https://arxiv.org/pdf/2402.07386.pdf)\] \[[Code](https://github.com/QingkaiZeng/Chain-of-Layer)\]
  
* **Qingkai Zeng**, Zhihan Zhang, Jinfeng Lin, Meng Jiang. Completing Taxonomies with Relation-Aware Mutual Attentions, In the Workshop on Mining and Learning with Graphs (**MLG**) in conjunction with the ACM SIGKDD international conference on Knowledge discovery and data mining (**KDD**), 2023. \[[PDF](https://www.mlgworkshop.org/2023/papers/MLG__KDD_2023_paper_10.pdf)\]

* **Qingkai Zeng**, Jinfeng Lin, Wenhao Yu, Jane Cleland-Huang, Meng Jiang, Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations,  In Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining (**KDD**), 2021 . \[[PDF](https://dl.acm.org/doi/pdf/10.1145/3447548.3467308)\]

* **Qingkai Zeng**, Wenhao Yu, Mengxia Yu, Tianwen Jiang, Tim Weninger, Meng Jiang, Tri-Train: Automatic Pre-fine Tuning between Pre-training and Fine-tune Training for SciNER, Findings of Empirical Methods on Natural Language Processing (**EMNLP**), 2020. \[[PDF](https://aclanthology.org/2020.findings-emnlp.429.pdf)\]

* **Qingkai Zeng**, Mengxia Yu, Wenhao Yu, Jinjun Xiong, Yiyu Shi, Meng Jiang, Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method, In the Workshop on Graph-Based Natural Language Processing (TextGraphs) at Conference on Empirical Methods in Natural Language Processing (**EMNLP**), 2019. \[[PDF](/papers/W1_TextGraph_2019.pdf)\]
  
* Zhaoxuan Tan, **Qingkai Zeng**, Yijun Tian, Zheyuan Liu, Bin Yin, Meng Jiang. Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning, arXiv preprint arXiv:2402.04401 (2024). \[[PDF](https://arxiv.org/pdf/2402.04401.pdf)\]
  
* Jinfeng Lin, Yalin Liu, **Qingkai Zeng**, Meng Jiang, Jane Cleland-Huang, Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models, in Proceeding of IEEE/ACM International Conference on Software Engineering (**ICSE**), 2021. \[[PDF](https://arxiv.org/pdf/2102.04411.pdf)\] (**<span style="color:red;">ACM SIGSOFT Distinguished Paper Award</span>**)

* Chenguang Zhu, William Hinthorn, Ruochen Xu, **Qingkai Zeng**, Michael Zeng, Xuedong Huang, Meng Jiang, Boosting Factual Correctness of Abstractive Summarization with Knowledge Graph,  In Proceedings of North American Chapter of the Association for Computational Linguistics (**NAACL**), 2021 . \[[PDF](/papers/C7_BoostingFactual_2020.pdf)\]

Contact
======
* Email: qzeng \[at\] nd \[dot\] edu
* Office: 355 Fitzpatrick Hall of Engineering
* Location: University of Notre Dame, Notre Dame, IN 46565

<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=150&t=tt&d=FuSGlaDDeSKr7GVQlb2C7DIXoOpnUVLDvwTxcFAjKeQ'></script>
<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=KUJT2QgVMVLmsoIjFca2050D_72VKBt9CGWpR-obfR0&cl=ffffff&w=a"></script>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=255&t=tt&d=kEZn9_SbA7ubXcpAQCCRQtlCn12JapFLcBzO8lEGt7g&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>

