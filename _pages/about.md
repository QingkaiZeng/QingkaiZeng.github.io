---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an Assitant Professor in the [College of Computer Science](https://cc.nankai.edu.cn/) at the [Nankai University](https://www.nankai.edu.cn/). Prior to joining Nankai University, I spent a rewarding year as a Postdoctoral Scientist at Amazon, where I worked with Professors [Rui Song](https://song-ray.github.io/), [Han Zhao](https://hanzhaoml.github.io/) at UIUC, Prof. Hengrui Cai at UCI and [Sheng Wang](https://homes.cs.washington.edu/~swang/) at the University of Washington on developing large language models for seller-domain tasks. Before that, I received my Ph.D. degree in Computer Science and Engineering at the [University of Notre Dame](https://cse.nd.edu), advised by [Prof. Meng Jiang](http://www.meng-jiang.com/). I also hold a Masterâ€™s degree in Electrical and Computer Engineering from [UIUC](https://ece.illinois.edu) and a Bachlor degree from Sun Yat-sen University. 

Research Interest
======
My primary research interest lies in data mining, natural language processing and machine learning. Basically, I define myself as an researcher whom mining the structural knowledge from heterogeneous resource data. I aim to use this knowledge as a first principle to enable intelligence systems to generate more accurate, trustworthy and explainable results, ultimately helping to reduce human effort. 

My current focus include several key areas: NLP with Structural Knowledge, NLP for Science, Information Retrieval, Retrieval-Augmented Generation, Large Language Model Reasoning. Below are some keywords that reflect my research interests:

* Structural Knowledge Construction
* Knowledge-enhanced Reasoning System
* Trustworthy Large Language Model

Mentoring & Collaborating
======
* [Xianrui Zhong](https://xianruizhong.github.io/), UIUC. We work on RAG with Structural Knowledge.
* [Zehong Wang](https://zehong-wang.github.io/), University of Notre Dame. We work on Agentic LLM.
* [Mengxia Yu](https://scholar.google.com/citations?user=9d9qJt8AAAAJ&hl=en), University of Notre Dame. We work on Agentic LLM.
* [Bolian Li](https://lblaoke.github.io/), Purdue University. We work on LLM alignment.
* [Yifan Wang](https://cacayaya.github.io/), Purdue University. We work on LLM alignment.
* [Yanjin He](), Peking University. Our work on science of LLM has been published on EMNLP 2025.
* [Yuyang Bai](https://leopoldwhite.github.io/), XJTU. Our works on taxonomy construction have been published on CIKM 2024 and ACL 2025.
* [Zhenyu Wu](https://scholar.google.com/citations?user=5tVLNpYAAAAJ&hl=zh-CN), XJTU. Our works on mathematical reasoning have been published on EMNLP 2024 and ACL 2025.
* [Zhaoxuan Tan](https://zhaoxuan.info/), University of Notre Dame. Our work on personalized LLM has been published on EMNLP 2024.
* [Billy Porter](https://scholar.google.com/citations?user=pmElWfwAAAAJ&hl=en), University of Notre Dame. We work on weakly-supervised text classifcation. Billy is now Researcher in Google.

My career has been supported and inspired by many role models, including all of my advisors listed above, and many others, including 

* [Prof. Meng Jiang](http://www.meng-jiang.com/), for shaping my research philosophy, cultivating my academic taste, and guiding me from a beginner to someone equipped with essential research skills. I truly believe that the highest recognition I could receive from the community is to be told that I pursue research in the spirit and integrity that you embodied.
* [Prof. Jiawei Han](https://hanj.cs.illinois.edu/), for always being a role model and for helping shape my research taste at the early stage of my academic career.
* [Prof. Xiangliang Zhang](https://engineering.nd.edu/faculty/xiangliang-zhang/), [Prof. Jane Cleland-Huang](https://engineering.nd.edu/faculty/jane-cleland-huang/), [Prof. Tim Weninger](https://engineering.nd.edu/faculty/tim-weninger/), [Prof. David Chiang](https://www3.nd.edu/~dchiang/) for their support and knowledge sharing during my Ph.D. studies at the University of Notre Dame.
* [Prof. Mark Allan Hasegawa-Johnson](https://ece.illinois.edu/about/directory/faculty/jhasegaw),  [Dr. Jiaming Shen](https://mickeysjm.github.io/) for their support with my application and career planning as I pursued a Ph.D. degree.
* [Dr. Dong Yu](https://sites.google.com/view/dongyu888/), [Dr. Linfeng Song](https://freesunshine0316.github.io/), [Dr. Lingfei Wu](https://scholar.google.com/citations?user=VYi6qHMAAAAJ&hl=en), [Dr. Xiaojie Guo](https://sites.google.com/view/xiaojie-guo-personal-site)  for their insightful industry knowledge and for shaping my perspective on how to think like an NLP researcher.

I also hold the deepest respect for [Prof. Thomas Huang](https://ifp-uiuc.github.io/), even though I never had the opportunity to be advised by him or study in his group or classes. His remarkable life and career have deeply inspired me, leading me to reflect thoughtfully on the academic path I hope to pursue and the kind of professor I strive to become.

What's New
======
* \[August 2024\] The [code](https://github.com/QingkaiZeng/CodeTaxo-Pub) of [CodeTaxo](https://www.arxiv.org/pdf/2408.09070) is available now. Feel free to generate your own taxonomy!
* \[August 2024\] Three papers about (1) Taxonomy Expansion via Code Prompting ([CodeTaxo](https://www.arxiv.org/pdf/2408.09070)) (2) Mathematical Reasoning ([ProCo](https://arxiv.org/pdf/2405.14092)) (3) Curruiculum Learning ([PUDF](https://arxiv.org/pdf/2408.05326)) are available on Arxiv now! 
* \[July 2024\] [Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf) was accepted by [The CIKM 2024](https://cikm2024.org/). The [code](https://github.com/QingkaiZeng/Chain-of-Layer) of [Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf) is available now. Feel free to generate your own taxonomy!
* \[February 2024\] Two papers about (1) Entity Linking via leveraging LLMs ([ChatEL (Coming Soon)]()) (2) Mathematical Reasoning ([MinT](https://arxiv.org/pdf/2307.07951.pdf)) were accepted by [LREC-COLING 2024](https://lrec-coling-2024.org/). Huge congrats to [Yifan Ding](https://scholar.google.com/citations?user=WMegVFUAAAAJ&hl=en) and [Zhenwen Liang](https://zhenwen-nlp.github.io/)!
* \[February 2024\] Three papers about (1) Taxonomy Induction via prompting LLMs ([Chain-of-Layer](https://arxiv.org/pdf/2402.07386.pdf)) (2) personalized LLMs ([OPPU](https://arxiv.org/pdf/2402.04401.pdf)) (3) Entity Linking via leveraging LLMs ([EntGPT](https://arxiv.org/pdf/2402.06738.pdf)) are available on Arxiv now! 
* \[October 2023\] One paper was accepted by [The EMNLP 2023](https://2023.emnlp.org/).
* \[April 2023\] I am joining Tencent AI Lab as a full-time research intern this summer! I will work with Dr. [Linfeng Song](https://freesunshine0316.github.io/) and Dr. [Haitao Mi](https://scholar.google.com/citations?user=G3OMbFSm858C&hl=en). 
* \[May 2022\] One paper was accepted by [The KDD 2022](https://kdd.org/kdd2022/).
* \[May 2021\] One paper was accepted by [The KDD 2021](https://www.kdd.org/kdd2021/).
* \[May 2021\] Our [ICSE 2021](https://conf.researchr.org/home/icse-2021) paper was selected for an ACM SIGSOFT Distinguished Paper Award!


Recent Publications
======

* **Qingkai Zeng**, Yuyang Bai, Zhaoxuan Tan, Zhenyu Wu, Shangbin Feng, Meng Jiang. CodeTaxo: Enhancing Taxonomy Expansion with Limited
Examples via Code Language Prompts, Findings of Annual Meetings of the Association for Computational Linguistics (**ACL**), 2025. \[[PDF](https://www.arxiv.org/pdf/2408.09070)\] \[[Code](https://github.com/QingkaiZeng/CodeTaxo-Pub)\]

* **Qingkai Zeng**, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang. Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples, In Proceedings of the ACM International Conference on Information & Knowledge Management (**CIKM**). \[[PDF](https://arxiv.org/pdf/2402.07386.pdf)\] \[[Code](https://github.com/QingkaiZeng/Chain-of-Layer)\]
  
* **Qingkai Zeng**, Zhihan Zhang, Jinfeng Lin, Meng Jiang. Completing Taxonomies with Relation-Aware Mutual Attentions, In the Workshop on Mining and Learning with Graphs (**MLG**) in conjunction with the ACM SIGKDD international conference on Knowledge discovery and data mining (**KDD**), 2023. \[[PDF](https://www.mlgworkshop.org/2023/papers/MLG__KDD_2023_paper_10.pdf)\]

* **Qingkai Zeng**, Jinfeng Lin, Wenhao Yu, Jane Cleland-Huang, Meng Jiang, Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations,  In Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining (**KDD**), 2021 . \[[PDF](https://dl.acm.org/doi/pdf/10.1145/3447548.3467308)\]

* **Qingkai Zeng**, Wenhao Yu, Mengxia Yu, Tianwen Jiang, Tim Weninger, Meng Jiang, Tri-Train: Automatic Pre-fine Tuning between Pre-training and Fine-tune Training for SciNER, Findings of Empirical Methods on Natural Language Processing (**EMNLP**), 2020. \[[PDF](https://aclanthology.org/2020.findings-emnlp.429.pdf)\]

* **Qingkai Zeng**, Mengxia Yu, Wenhao Yu, Jinjun Xiong, Yiyu Shi, Meng Jiang, Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method, In the Workshop on Graph-Based Natural Language Processing (TextGraphs) at Conference on Empirical Methods in Natural Language Processing (**EMNLP**), 2019. \[[PDF](/papers/W1_TextGraph_2019.pdf)\]
  
* Zhaoxuan Tan, **Qingkai Zeng**, Yijun Tian, Zheyuan Liu, Bin Yin, Meng Jiang. Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning, arXiv preprint arXiv:2402.04401 (2024). \[[PDF](https://arxiv.org/pdf/2402.04401.pdf)\]
  
* Jinfeng Lin, Yalin Liu, **Qingkai Zeng**, Meng Jiang, Jane Cleland-Huang, Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models, in Proceeding of IEEE/ACM International Conference on Software Engineering (**ICSE**), 2021. \[[PDF](https://arxiv.org/pdf/2102.04411.pdf)\] (**<span style="color:red;">ACM SIGSOFT Distinguished Paper Award</span>**)

* Chenguang Zhu, William Hinthorn, Ruochen Xu, **Qingkai Zeng**, Michael Zeng, Xuedong Huang, Meng Jiang, Boosting Factual Correctness of Abstractive Summarization with Knowledge Graph,  In Proceedings of North American Chapter of the Association for Computational Linguistics (**NAACL**), 2021 . \[[PDF](/papers/C7_BoostingFactual_2020.pdf)\]

Contact
======
* Email: qzengnkcs \[at\] gmail \[dot\] com
* Office: TBD
* Location: TBD

<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=150&t=tt&d=FuSGlaDDeSKr7GVQlb2C7DIXoOpnUVLDvwTxcFAjKeQ'></script>
<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=KUJT2QgVMVLmsoIjFca2050D_72VKBt9CGWpR-obfR0&cl=ffffff&w=a"></script>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=255&t=tt&d=kEZn9_SbA7ubXcpAQCCRQtlCn12JapFLcBzO8lEGt7g&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>

